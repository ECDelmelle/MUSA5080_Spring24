---
title: "MUSA 508, Lab 5 - Spatial Machine Learning Pt. 2"
author: "Harris, Fichman and Steif - 2023"
date: '2023-09-26'
output: html_document
---

Updated by E.C.Delmelle March 2024

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(spdep) #for spatial autocorrelation
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(jtools)     # for regression model plots
library(broom)
library(tufte)
library(rmarkdown)
library(kableExtra)
library(tidycensus)
library(RColorBrewer)
# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Learning objectives

- Build a training and testing environment to validate a regression model

- Examine Errors to understand if they are non-random, in order to diagnose model quality

- Create and examine validation metrics including MAE and MAPE

- Use Moran's I to see if errors are spatially non-random

- Examine generalizability across geographic contexts

## Data Loading

Bring in a wrangled data set - it has some new variables already created.

```{r read_data, results='hide'}
#https://github.com/urbanSpatial/Public-Policy-Analytics-Landing/tree/master/DATA/Chapter3_4


boston.sf <- st_read(file.path(root.dir,"/Chapter3_4/boston_sf_Ch1_wrangled.geojson")) %>% 
  st_set_crs('ESRI:102286')

nhoods <- 
  st_read("https://raw.githubusercontent.com/mafichman/musa_5080_2023/main/Week_4/neighborhoods/bost_nhoods.geojson") %>%
  st_transform('ESRI:102286')

```

## Split Data into Train/Test Set

Let's begin by doing a regression on the Boston Housing Dataset to predict house sale prices.
Here are some details on what the code does:


'createDataPartition()': This function from the 'caret' package is used to split the dataset into a training set ('boston.training') and a test set ('boston.test'). The y parameter specifies the outcome variable used for stratified sampling, which is created by concatenating the values of several columns (Name, NUM_FLOORS.cat, Style, R_AC). By concatenating these columns, we create a unique identifier for each observation in the dataset. This unique identifier will be used to ensure that observations with similar characteristics are grouped together in either the training or test set.

The 'p' parameter specifies the proportion of data to include in the training set (in this case, 60%). The list parameter is set to FALSE to return a logical vector instead of a list.

'createDataPartition()' is used to divide the dataset into training and test sets while ensuring that observations with similar characteristics are evenly distributed between the two sets. This helps to create more reliable models by ensuring that the model is trained and tested on a representative sample of the data.

'boston.training' and 'boston.test': These are subsets of the original dataset ('boston.sf') based on the indices generated by 'createDataPartition()'. 'boston.training contains 60%' of the data for training the regression model, while 'boston.test' contains the remaining 40% for evaluating the model.

'lm()': This function fits a linear regression model to predict the 'SalePrice' using predictors such as 'LivingArea', 'Style', 'GROSS_AREA', 'NUM_FLOORS.cat', 'R_BDRMS', 'R_FULL_BTH', 'R_HALF_BTH', 'R_KITCH', 'R_AC', 'R_FPLACE', 'crimes.Buffer', and 'Name'. The model is trained using the data in 'boston.training'.

Model Evaluation: After fitting the model, predictions are made on the test set ('boston.test'). These predictions are stored in the 'SalePrice.Predict' column. The code then calculates several error metrics ('SalePrice.Error', 'SalePrice.AbsError', 'SalePrice.APE') to evaluate the performance of the model. 'APE' stands for Absolute Percentage Error, which is the absolute difference between the predicted and actual sale prices divided by the actual sale price. Finally, any records with a SalePrice greater than $5,000,000 are filtered out of the test set.


```{r traintest}

# Split the dataset into a training set and a test set using stratified sampling
inTrain <- createDataPartition(
              y = paste(boston.sf$Name, boston.sf$NUM_FLOORS.cat, 
                        boston.sf$Style, boston.sf$R_AC), 
              p = .60, list = FALSE)  # Create a vector of indices for the training set

# Subset the dataset to create the training set
boston.training <- boston.sf[inTrain,]  # Training set
# Subset the dataset to create the test set
boston.test <- boston.sf[-inTrain,]     # Test set
 
# Fit a linear regression model to predict SalePrice using selected predictors
reg.training <- 
  lm(SalePrice ~ ., data = as.data.frame(boston.training) %>% 
                             dplyr::select(SalePrice, LivingArea, Style, 
                                           GROSS_AREA, NUM_FLOORS.cat,
                                           R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                           R_KITCH, R_AC, R_FPLACE, crimes.Buffer))

# Make predictions on the test set and evaluate model performance
boston.test <-
  boston.test %>%  # Pipe the test set into the following operations
  # Add a column indicating the type of regression model used
  mutate(Regression = "Baseline Regression",
         # Predict sale prices using the trained regression model
         SalePrice.Predict = predict(reg.training, boston.test),
         # Calculate the difference between predicted and actual sale prices
         SalePrice.Error = SalePrice.Predict - SalePrice,
         # Calculate the absolute difference between predicted and actual sale prices
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         # Calculate the absolute percentage error
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice) %>%
  filter(SalePrice < 5000000)  # Filter out records with SalePrice greater than $5,000,000


```

What is the MAE?

```{r}
mean(boston.test$SalePrice.AbsError, na.rm = T)

```

How about the MAPE?

```{r}
mean(boston.test$SalePrice.APE, na.rm = T)

```

## Cross-Validation

To set up an iteration of random 'folds' (in this case 100), we use our very same regression formula to train a different regression object `reg.cv` that is subjected to the cross-validation process for the whole data set.

This model uses the 'train()' function from the 'caret' package.

This line trains a linear regression model using the train() function from the caret package.

If you get warnings about a "rank deficient" matrix - you might have too many predictors for the number of observations.

```{r crossvalidation}
# Set up cross-validation for model evaluation
fitControl <- trainControl(method = "cv", number = 100)

# Set the seed for reproducibility
set.seed(825)

# Train a linear regression model using cross-validation
reg.cv <- 
  train(SalePrice ~ ., 
        data = st_drop_geometry(boston.sf) %>% 
               dplyr::select(SalePrice, LivingArea, Style, GROSS_AREA, 
                             NUM_FLOORS.cat, R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                             R_KITCH, R_AC, R_FPLACE, crimes.Buffer), 
        method = "lm",  # Specify the modeling method as linear regression
        trControl = fitControl,  # Specify the cross-validation settings
        na.action = na.pass)  # Specify how to handle missing values

# View the results of the cross-validated linear regression model
reg.cv


```
`reg.cv` is an object that contains a data frame called `resample` with your folds and errors associated with them. You will use this to make a plot of your errors for the CV process.
Each row in the resample data frame represents the performance of the model on a specific fold of the cross-validation process.

```{r displayCV}
# Display the first 5 rows of the 'resample' data frame within the 'reg.cv' object
reg.cv$resample[1:5,]


```


## Spatial Lags

What is the relationship between errors? Are they clustered? Is the error of a single observation correlated with the error of nearby observations?

We create a list of "neighbors" using a "spatial weights matrix".

```{r spatiallags}
# Extract the coordinates from the 'boston.sf' spatial dataframe
coords <- st_coordinates(boston.sf) 

# Create a neighbor list using k-nearest neighbors (KNN) with k=5
neighborList <- knn2nb(knearneigh(coords, 5))

# Convert the neighbor list to a spatial weights matrix
spatialWeights <- nb2listw(neighborList, style="W")

# Compute the spatial lag of the variable 'SalePrice' using the spatial weights matrix
boston.sf$lagPrice <- lag.listw(spatialWeights, boston.sf$SalePrice)


```

Then, we calculate the spatial lag of the errors and plot them versus the regression errors.

```{r}
# Extract the coordinates of the test dataset
coords.test <- st_coordinates(boston.test) 

# Create a neighbor list using k-nearest neighbors (KNN) with k=5 for the test dataset
neighborList.test <- knn2nb(knearneigh(coords.test, 5))

# Convert the neighbor list to a spatial weights matrix for the test dataset
spatialWeights.test <- nb2listw(neighborList.test, style="W")
 
# Compute the spatial lag of the 'SalePrice.Error' variable for each observation in the test dataset
# by averaging the values of 'SalePrice.Error' for its nearest neighbors using the spatial weights matrix,
# and add the result as a new variable 'lagPriceError' to the test dataset
boston.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%
  
  # Create a scatter plot to visualize the relationship between 'lagPriceError' and 'SalePrice.Error'
  ggplot()+
  geom_point(aes(x = lagPriceError, y = SalePrice.Error))


```

## Do Errors Cluster? Using Moran's I

So - is your Moran's I statistic indicating dispersion (-1), randomness (0) or clustering (1)?


```{r}
moranTest <- moran.mc(boston.test$SalePrice.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  theme_minimal()


moranTest$statistic
```

## Predictions by neighborhood
How are the errors distributed by neighborhood?

```{r predictbyneighborhood}
# This code chunk performs several operations on the 'boston.test' dataset and formats the result into a visually appealing table using 'kable' and 'kable_styling'.

# Convert the 'boston.test' dataset to a regular data frame
boston.test %>%
  as.data.frame() %>%

  # Group the data frame by the 'Name' variable
  group_by(Name) %>%
  
  # Calculate the mean of 'SalePrice.Predict' and 'SalePrice' variables within each group
  summarize(meanPrediction = mean(SalePrice.Predict),
            meanPrice = mean(SalePrice)) %>%
  
  # Format the summarized data frame as an HTML table
  kable() %>%
  
  # Apply styling to enhance the appearance of the HTML table
  kable_styling()


```

## Regression with neighborhood effects

Let's try to run the regression again, but this time with a neighborhood fixed effect. We do this by simply adding the variable 'Name' which is the neighborhood to the regression model as a neighborhood-level dummy variable.

```{r}
# This code fits a linear regression model ('reg.nhood') to predict 'SalePrice' using various predictors, including neighborhood-related variables, using the training dataset.

# Fit a linear regression model to the training dataset ('boston.training')
reg.nhood <- lm(SalePrice ~ ., 
                data = as.data.frame(boston.training) %>% 
                         dplyr::select(Name, SalePrice, LivingArea, 
                                       Style, GROSS_AREA, NUM_FLOORS.cat,
                                       R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                       R_KITCH, R_AC, R_FPLACE, crimes.Buffer))
#View the model stats
summary(reg.nhood)

# Create a new dataset ('boston.test.nhood') by predicting 'SalePrice' for the test dataset ('boston.test') using the neighborhood effects model ('reg.nhood')
boston.test.nhood <-
  boston.test %>%
  mutate(Regression = "Neighborhood Effects",  # Add a new variable indicating the type of regression model
         SalePrice.Predict = predict(reg.nhood, boston.test),  # Predict 'SalePrice' using the fitted model
         SalePrice.Error = SalePrice.Predict - SalePrice,  # Calculate the error between predicted and actual 'SalePrice'
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),  # Calculate the absolute error
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice) %>%  # Calculate the absolute percentage error
  filter(SalePrice < 5000000)  # Filter out observations with 'SalePrice' greater than $5,000,000


```

How do these models compare? We can bind our error info together and then examine!

This code snippet concatenates the results from two different regression models ('boston.test' and 'boston.test.nhood') into a single dataset named 'bothRegressions'. It first selects relevant variables from each dataset and then computes the spatial lag of the 'SalePrice.Error' variable using the provided spatial weights matrix ('spatialWeights.test'). Finally, it binds these two datasets row-wise using the 'rbind' function.

```{r compareregressions}
bothRegressions <- 
  rbind(
    dplyr::select(boston.test, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(boston.test.nhood, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))  

```

Create a table showing the difference between the two regressions.

```{r}
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -Name) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable()
```

## Further examination of errors

Predicted versus observed plots - what does it mean if the line is above or below y=x?

```{r}
bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
    ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(SalePrice.Predict, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  theme_minimal()
```

We can also examine the spatial pattern of errors.

```{r}
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, Name) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(nhoods, by = c("Name" = "neighborhood")) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = mean.MAPE)) +
      geom_sf(data = bothRegressions, colour = "black", size = .5) +
      facet_wrap(~Regression) +
      scale_fill_gradient(brewer.pal(7, "Blues"),
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood") +
      theme_void()

```

## Race and income context of predictions

What is the race and income context of Boston census tracts, and how does this relate to our model performance?

```{r}
tracts17 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2017, state=25, county=025, geometry=T, output = "wide") %>%
  st_transform('ESRI:102286')  %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Race Context") +
    theme_void()+ theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    theme_void() + theme(legend.position="bottom"))

```

```{r}
st_join(bothRegressions, tracts17) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood income context")
```